{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Jupyter notebooks and Pandas\n",
    "\n",
    "A Jupyter notebook (formally iPython) is an interactive environment for Python, and it's probably the best way of using Python for data manipulation.  You may ask: \"I can just run python interactively from the terminal, why do I need jupyter?\"  Well, that's a fair question, and the answer will hopefully become clear as we work through this notebook.\n",
    "\n",
    "Jupyter notebooks are broken down into **cells**.  We're in the topmost cell of this notebook at the moment.  Cells come in three flavors:\n",
    "\n",
    "* **Markdown cells** allows you to edit the text in [Markdown](https://guides.github.com/features/mastering-markdown/).  These cells are used for exposition, discussion, and general formatting.  Think of them as extended comments that can be formatted beautifully, and can contain [links](http://www.jupyter.org), bulleted lists, etc.  Anything that Markdown can!  They can even contain $\\LaTeX$ code ([huh?](#footer_latex)): $$\\left(\\int_1^{\\sqrt[3]{3}}z^2 dz \\right)\\cos\\left(\\frac{3\\pi}{9}\\right) = \\ln\\left(\\sqrt[3]{e}\\right)$$\n",
    "* **Code cells** contain code (for us, Python code).  These cells can contain code as short as one line, or as long as you'd like!  (Actually, I have no idea what the maximum length is.  I've had cells well over 200 lines long though).  They have some basic text editor support, so they'll help you with indentation, tab completion, etc., but they won't be able to do some of the magic that true editors like Atom or Sublime can handle.  They're also interactive in the same way that the Python interpretter in interactive mode is.  Type `15*4 %3` and it returns the answer, no need to print out everything.\n",
    "\n",
    "There's one more, but it's not used as often:\n",
    "\n",
    "* **Raw cells** are used when you want to hack the notebook to make it fancier.  We won't be using them, but it's good to know they exist.\n",
    "\n",
    "How about a little demonstation?  Try running the cell below by highlighting it and pressing either `cmd + enter` or `ctrl + enter`.  If that *doesn't* work, then you're not [running the notebook](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/execute.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    \"\"\" Determine whether n is prime.\"\"\"\n",
    "    k = 2\n",
    "    while k*k <= n:\n",
    "        if n % k == 0:\n",
    "            return False\n",
    "        k += 1\n",
    "    return True\n",
    "\n",
    "print(*[x for x in range(2,401) if is_prime(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Side note** for people confused by that last line](#footer_list_comprehensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editting a notebook: Command mode and Edit mode\n",
    "\n",
    "While working with a notebook, you are always in one of two modes.\n",
    "\n",
    "1. In **Edit mode** you can edit the content of a cell.  It acts like a text file inside a text editor, and has some helpful syntax highlighting.  If you're editing a Markdown cell, it will look significantly different.  If you're editing a code cell, it will look mostly the same.  To *run* the cell, you have a few options:\n",
    " * press `command + enter` or `ctrl + enter` to run the cell and exit edit mode.  Running a markdown cell will render it, and running a code cell performs as you expect.\n",
    " * press `shift + enter` to run the cell and insert a new cell below.  This is the standard command when you're building the notebook.\n",
    " * You can also press `esc` to go to command mode without running the cell.\n",
    "1. In **Command mode** you have access to your cells in a larger-scale way.  You can press `up` or `down` to move between cells, and press `enter` to enter edit mode on the currently selected cells.  You can also cut, copy, paste, and delete cells with appropriate keyboard commands.  Open the *Command Palette* (the keyboard in the top center of the toolbar) to see all the commands you can use in Command mode.\n",
    "\n",
    "## Linearity of code: the kernel\n",
    "\n",
    "A notebook has a **kernel** attached to it.  Think of it as the interactive python running behind the scenes, executing your commands when you send them.  There are two forms of *linearity*, or continuity-of-your-work, going on here, and it can be a bit confusing to new Jupyter users:\n",
    "\n",
    "* **Kernel Linearity**: After you execute a code cell, it gives you its output and places a number next to the top left corner of the cell.  This number is the *order of cell execution* in the kernel.  It's the order the kernel received from you.  This means you can run cells, tweak them and run them again, run something \"below\" the cell in the notebook, then come back and run the upper cell, *etc.*, and the kernel will keep track of this in terms of the order in which you ran them **chronologically**.  This is the order you want to keep in mind.  It's really useful!  You can start out with a junky-looking notebook, figure out your data analysis, realize you want to change stuff \"in the past\", and just go back and change them.  Once you get used to this, you'll love it.\n",
    "* **Cell Linearity**: There is an obvious order to the cells: the top ones \"go first\", and the lower ones \"go next\".  This isn't exactly necessary, though.  It definitely is the goal of the *final product* to go linearly, but programming, and especially data analysis, isn't like writting a journal entry.  Very often, you'll need to go back and change things, then rerun all the cells that come after the one you just edited.  You may type one line in a cell, hit `shift + enter` to see the output and move on to the next cell, then do that three more times.  You then realize that you'd prefer to have done all that at once, and you can merge those three cells together.  It's a workflow that I hope you'll learn to love.\n",
    "\n",
    "Play around with it now: we first use an uninitialized variable `my_hat` in a cell; hit `shift + enter` to see the error.  Below that, we create a cell in which we give the variable a value, then run that cell, followed by the original cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Push shift+enter to see an error or esc to not run the cell:\n",
    "\n",
    "print(my_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_hat = \"Oh, now it works!\"\n",
    "\n",
    "# run this cell, then run the above cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll get the hang of it in time. Another way this can bite you is by rerunning the same cell and expecting a certain result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell once!\n",
    "title = \"The Cat in the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell many times!\n",
    "\n",
    "title += \" Hat\"\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing to note about jupyter notebooks is that, unlike the Python interpretter, you have full access to your shell (bash or cmd, most likely) by using the `!` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo \"Hello from a text file!\" > hello.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's see the file we just made:\n",
    "\n",
    "# I'm on a windows machine as I make this notebook, so I'll use: \n",
    "!dir \n",
    "\n",
    "# But if you're on a *nix machine (such as a macbook) you should use: \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If there's no `!`, then we're using Python.  Here's the\n",
    "# Python command to open the text file we just made:\n",
    "with open(\"hello.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again, the windows commands: \n",
    "!del hello.txt\n",
    "\n",
    "# But if you're on a *nix machine you should use: \n",
    "# !rm hello.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can now go run the above `!dir` or `!ls` cell to double check that the file is gone!  Remember, *kernel linearity* is the imporant thing to keep in mind.)  I mostly use `!` in order to `!pip install <whatever_python_library_I_need>`.  (What's [pip](https://pythonprogramming.net/using-pip-install-for-python-modules/)?) \n",
    "\n",
    "Now let's move on to something more interesting.\n",
    "\n",
    "# A basic data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standard import statements for data analysis\n",
    "import numpy as np                  # linear algebra library\n",
    "import pandas as pd                 # data analysis and manipulation\n",
    "import matplotlib.pyplot as plt     # standard-issue plotting library\n",
    "import seaborn as sns               # fancier plotting library\n",
    "\n",
    "# other helpful libraries\n",
    "import requests                     # allows the use of HTTP requests\n",
    "from io import StringIO             # turns a string into an input stream so that pandas can load it.\n",
    "from os.path import join            # allows for operating-system-specific path joining\n",
    "\n",
    "# The following line is a Jupyter \"magic\": lines beginning with a `%` are how you talk to Jupyter \n",
    "#   (instead of Python or the shell) .  Here, I'm telling Jupyter to display matplotlib plots \n",
    "#   as inline, as opposed to the default of having them pop up in their own window, buried \n",
    "#   behind everything else.\n",
    "%matplotlib inline\n",
    "\n",
    "# Use the Requests library to pull a dataset from the internet\n",
    "response = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
    "\n",
    "# Get just the data \n",
    "data_string = response.text\n",
    "\n",
    "print(response.text[:279])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a mess!  Let's turn that into something a bit more manageable.  Enter pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `df` is short for dataframe, the standard object in Pandas (which we abbreviated as `pd`), the Python \n",
    "#    dataset manipulation library.  Think of it as Excel, but awesome.\n",
    "df = pd.read_csv(StringIO(data_string))\n",
    "\n",
    "# show me the top 5 rows!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it's prettier, but the columns are all messed up.  What gives?  Pandas figures that your first row is the names of the columns.  We'll add an extra line to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"species\"]\n",
    "\n",
    "# read it in again to not lose the first line of data:\n",
    "df = pd.read_csv(StringIO(data_string), names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now this is a working dataset.  Let's do some basic checks on our data, just to see what we're working with.\n",
    "\n",
    "(By the way, a quick *meta note*: notice that I'm using these markdown cells to walk you through the process of my data analysis using this notebook.  It's **much** nicer to look through someone else's work and see what they've done when they tell you a bit of a story with their Markdown cells along the way!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pandas commands: data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in our little `df.head()` call above, all the entries for `species` were the same.  That suggests there aren't too many *unique* options for `species` (In this case, `species` is *categorical data*: it doesn't really have any ordering).  How many unique options are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['species'].unique()\n",
    "\n",
    "# note that I could have also said:\n",
    "# df.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is the famous [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset.  It contains data on  three species of iris, with four interesting columns of data about them.  You can use this dataset to learn about plotting, making basic machine learning models, etc.  Let's continue to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some useful statistics on the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you quite a bit about the dataset, if you understand what you're looking at.  `count` is the number of elements, `mean` is the mean of the data (note that this requires doing some math on the data, so it will often be messed up if you have an error in your dataset, like missing data), `std` is the standard deviation, and the rest are [percentiles](https://en.wikipedia.org/wiki/Percentile). From this, I can see that nothing looks our of place, like there are unlikely to be incorrect values ($-150$cm long petals, for example), so we're okay to move on to some *exploratory* plots of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic plotting\n",
    "As I mentioned above, Matplotlib is the standard plotting library for Python. Let's make a simple scatter plot of two of the columns, to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: the semi-colon is optional and makes the output slightly prettier.\n",
    "plt.scatter(df['petal length'], df['petal width']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, cool, that was pretty easy.  However, it's not currently telling me much about that important-seeming lower left chunk of data points.  Is it maybe just one species, and the upper bit is the other two?  We can add some color to our plot to test this hypothesis.  To do that, we need to feed a list of colors that's the same length as the data (a length-150 list of strings like `\"red\", \"blue\"`, *etc.*) based on the species.  We'll make a Python dictionary to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_options = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# overly fancy for this example, but very useful when you have tons of categories in your variable!\n",
    "color_mapping = {species: color for species, color in zip(df['species'].unique(), color_options)}\n",
    "\n",
    "colors = [color_mapping[species] for species in df['species']]\n",
    "\n",
    "print(*colors, sep = \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty long and ugly printout.  I usually wouldn't leave that in the notebook, but I'm showing you what we're creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['petal length'], df['petal width'], c=colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis was correct!  Awesome!  But, which species does this single out?  \"The red one\" is a pretty unacceptable answer; we need a legend!  Here's a first guess: the matplotlib documentation says that `plt.legend` needs \"labels\", whatever that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['petal length'], df['petal width'], c=colors)\n",
    "plt.legend(labels=df['species'].unique());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's... not right, and a bit silly.  It turns out that in order to do this with basic matplotlib, we actually have to split our data up based on its species (category).  (You're probably thinking \"Wait, really? That's a silly way to do it!\", and you wouldn't be wrong.  Stay with me!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for species in df['species'].unique():\n",
    "    # Restrict the dataset to just those rows that are this species\n",
    "    df_restricted = df.loc[df['species'] == species]\n",
    "    \n",
    "    # Note that you can also just put in a single color to the `c` option:\n",
    "    plt.scatter(df_restricted['petal length'], \n",
    "                df_restricted['petal width'], \n",
    "                label=species,                  # This allows the legend to work\n",
    "                c=color_mapping[species])\n",
    "\n",
    "# Since we labeled the individual calls to `plt.scatter`, we can just use:\n",
    "plt.legend()\n",
    "\n",
    "# Always label your axes!\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, *that's* a nice graph.  But it sure was a decent amount of work to get it!  The library Seaborn (which is just a wrapper around Matplotlib with a bunch of convenience functions) has some helpful tools to get us this very straightforward graph more quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot('petal length', \n",
    "           'petal width', \n",
    "           data=df, \n",
    "           hue='species',       # color the data by the 'species' column\n",
    "           fit_reg=False,       # don't bother fitting a linear regression line\n",
    "           legend_out=False);   # place the legend inside the graph, not outside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last **awesomely useful** plot that Seaborn can make for you, especially for machine learning applications, is called a [pair plot](https://www.quora.com/What-are-pair-plots).  It compares all the different columns/variables against each other using scatter plots, and then plots histograms along the \"diagonal\", where the \"row\" and \"column\" variables of the plot are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with messy data\n",
    "\n",
    "Pandas is also really good at helping you if you have messy data.  The iris dataset that we've been working with is *famously* a clean, perfect dataset.  However, most are not, and certainly not the ones that you construct yourself.\n",
    "\n",
    "In the Fall of 2016, I surveyed my students for a Data Mining course regarding which datasets they might be interested in using.  I then tabulated this data and shared it with them.  We used it to introduce Pandas to the class.  Let's revisit it now, since it's such a great example of messy data!  I've changed it in two ways:\n",
    "\n",
    "1. I removed the names and slightly anonymized the data.\n",
    "1. I artifically made it slightly messier so that we can see a few more common data cleaning steps.\n",
    "\n",
    "<!-- Small side note: notice I used \"1.\" for both items and it updated automatically.  That's the beauty of markdown!  \n",
    "\n",
    "By the way, if you're reading this, you've entered edit mode for this cell.  I wanted to call attention to the thing above, but only if you can see it, so I wanted to add THIS text only if you're in edit mode.  How is this possible?  Well, markdown is secretly just pre-processed HTML, and this text is inside of an HTML comment, so that's why you can't see it in the final version of the cell.  The more you know! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure the file `raw_fall_2016_survey_data.csv` is in a subdirectory of this\n",
    "# directory called \"data\".\n",
    "\n",
    "df = pd.read_csv(join(\"data\", \"raw_fall_2016_survey_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used [`os.path.join`](https://docs.python.org/2/library/os.path.html#os.path.join) to make sure that the file opens no matter which operating system you are using, dear reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, that first column looks useless to me, like the csv has an index built in as a column, but I already have one.  Let's reload the dataset with that in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(\"data\", \"raw_fall_2016_survey_data.csv\"), index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now I see a \"`...`\" in there, because there are too many columns to view them all.  What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(*df.columns, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so my dataset comes from the students' dream job, their clubs, and their rating of all the datasets I offered them as options.  Cool.  \n",
    "\n",
    "I see a few problems here:\n",
    "\n",
    "1. People didn't call all the clubs the same thing, and \n",
    "2. It has missing data.\n",
    "\n",
    "The problem of what to do with [missing data](https://en.wikipedia.org/wiki/Missing_data) is a *huge* one, and entire branches of statistics have been set up to deal with it.  We'll get to it in a bit.\n",
    "\n",
    "One thing you could say, by the way, is that we're going to \"tidy\" the dataset.  The term [tidy data](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) means something very specific, and is a good, industry standard goal to work toward in your data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, time to get our hands dirty:\n",
    "\n",
    "### Cleaning up clubs:\n",
    "\n",
    "The first thing we need to do is get a handle on the issue.  Let's take a look at just the three \"club\" columns, they were the three slots on the survey that I had for students to enter the clubs in which they participate.  We can slice the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can access the three columns like this.\n",
    "df[[\"Club 1\", \"Club 2\", \"Club 3\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're following along at home exactly the way I hope you are, you should have just gotten an error:\n",
    "\n",
    "```\n",
    "KeyError: \"['Club 2'] not in index\"\n",
    "```\n",
    "\n",
    "That seems strange, because when I did `df.columns`, I saw it there.  Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(*[column_name for column_name in df.columns if \"Club\" in column_name], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, hm.  That column looks like it has an extra space in it.  Let's fix that, and any other weirdly-spaced columns along the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the function <my_string>.strip() is what you're looking for here:\n",
    "\"       \\t   \\t \\t No more beginning or ending spaces!       \".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [column.strip() for column in df.columns]\n",
    "print(*[column for column in df.columns if \"Club\" in column], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's good to go.  But you know what, I don't really like the fact that there are *any* spaces in the column names, including separating words.  Why?  First of all, it's non-standard.  Second of all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notice that the column named `Period` has no spaces in its name.  This allows me to say:\n",
    "print(df.Period.unique()) # unique() just collects the unique items from that column (or any list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Okay, so I had two sections of that class.)  If I wanted to do the same thing with another column, like `\"wireless str\"`, I would need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df[\"wireless str\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a little uglier.  So, let's *CamelCase* it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [col.title().replace(\" \", \"\") if \" \" in col else col for col in df.columns]\n",
    "\n",
    "print(*df.columns, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome.  \n",
    "\n",
    "#### Back to clubs\n",
    "\n",
    "Now let's try that original command regarding the clubs again.  Note that in order to perform a [slice](https://pandas.pydata.org/pandas-docs/stable/indexing.html) of the data involving multiple columns, you need to give the indexing command `df[<blah>]` a list of column names, hence the double brackets `[[...]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[[\"Club1\", \"Club2\", \"Club3\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting.  Let's make a list of all clubs.  By the way, the \"NaN\" columns stand for [not a number](https://en.wikipedia.org/wiki/NaN); they indicate missing data.  In this situation, it's because that student didn't report three clubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "club_columns = [\"Club1\", \"Club2\", \"Club3\"]\n",
    "\n",
    "clubs = []\n",
    "\n",
    "# This is sort of an ugly way to grab all these clubs.  There are many other options!\n",
    "for _, club in df[club_columns].iterrows():\n",
    "    clubs.extend([club[i] for i in range(3) if pd.notnull(club[i]) and club[i] not in clubs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(*sorted(clubs), sep = ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there are a few ways to deal with this.  Here are the two extremes:\n",
    "\n",
    "* **Group the ones obviously referring to the same clubs, then create indicator columns for each club**.  An *indicator column* is just a 1 or 0 based on whether a given student is in the given club.  This has quite an obvious, important benefit:  You keep all of the *signal* (information) contained in the dataset, and so that might be helpful for us in our analysis.  However, it has the effect of greatly increasing the \"width\" of your dataset (the number of \"columns\", or \"predictors\", or \"variables\"), which may make the dataset intractible.  With a dataset of this tiny size, I'm not too concerned about that.  The other potential issue is that you're not taking into effect that some of these clubs are very similar to each other, and some are very different.  By making these indicators weighted the same, you may be capturing a great deal of *noise*, a word meaning the random fluctuations that comes from data.  This is sometimes called *one-hotting*.\n",
    "* **Create two classes of clubs, based on something you may think is relevant.**  For example, I may make \"techy\" clubs and \"non-techy\" clubs.  This has the benefit of not having as much noise, but it also loses some signal.  This is often called *binning*.\n",
    "\n",
    "What you choose is up to you, and it's mostly a matter of how much time you have to go through all the ways of modeling your data.  You could have the motto of \"always try everything\", and that may work out for you in terms of getting the best analysis.  Or you may just want a \"good enough\" result, and roll with it.  There's really no right or wrong answer; it's really a matter of how much time you want to spend with this particular dataset and how much noise you can tolerate.\n",
    "\n",
    "For now, I'll take a middle ground, and I'll hard code these into a few groups.  It's still binning, but I'll use 6 bins instead of 2.  There really isn't that much easier of a way to do this without signicant natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "club_groups = {'philo/poli/econ': ['model UN','philo/debate', 'investment club','Democracy project',\n",
    "                                   'economics society','mock trial','debate','filo'],\n",
    "               'social': ['out of the blue', 'ecoaction','GSA','WOFO','SLAM/dance',\n",
    "                          'quiz bowl','band','movie makers'],\n",
    "               'PA duties' : ['tour guide','student activities','phillipian','peer tutor'],\n",
    "               'international': ['german', 'indopak','international club',],\n",
    "               'math/sci/tech': ['gaming', 'astronomy club','math','blue moon (sci)',\n",
    "                                 'cs club','techmasters','makers','astronomy','aviation club',\n",
    "                                 'chess','robotics'],\n",
    "               'entreprenuerial': ['big ideas club', 'tang', 'entrpreneurs club']}\n",
    "\n",
    "for group, clubs in club_groups.items():\n",
    "    df[group] = df[club_columns].isin(clubs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head() # Scroll way to the right to see our work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up Dream jobs\n",
    "\n",
    "Same story for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dream_jobs = []\n",
    "\n",
    "for job in df['DreamJob']:\n",
    "    if pd.notnull(job): \n",
    "        dream_jobs.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_groups = {'jobs_tech':['Google R&D', 'tech entrepreneur','Developer'],\n",
    "              'jobs_science': ['biomedical engineer','math-related','research scientist', \n",
    "                          'JPL operations engineer','neuroscientist'],\n",
    "              'jobs_business': ['CEO', 'product manager','venture capital',],\n",
    "              'jobs_fantastic': ['rollerskating waitress']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for group in job_groups:\n",
    "    df[group] = df.DreamJob.isin(job_groups[group]).apply(lambda x: {False:0, True:1}[x])\n",
    "\n",
    "df.head(15) #scroll way right one more time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleanup: create data matrix\n",
    "\n",
    "The last step is to remove all string columns, creating just a [matrix](https://en.wikipedia.org/wiki/Matrix_%28mathematics%29).  Let's inspect the data types (or `dtypes`) of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we can filter the columns by their `dtypes`.  You might think that this will be a huge task of typing in each column name that has type `object` in it, but of course there's tools to make your life easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done cleaning a dataset, it's usually good to save the data to a **new** file.  It's very important that it's new, to help with [data lineage/provenance](https://en.wikipedia.org/wiki/Data_lineage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.to_csv(join(\"data\",\"dataset_preferences.csv\"))\n",
    "\n",
    "# This way, if screw up something below and lose some data, you can always uncomment this:\n",
    "#X = pd.read_csv(join(\"data\",\"dataset_preferences.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The point of this notebook isn't to dive deeply into the analysis, so we'll just do one last little bit to show that we're ready to dive into more mathematical/statistical techniques.  The first thing we want to do is see if we have some missing data.\n",
    "\n",
    "The way we're going to zero in on it is to use what's called *boolean slicing*.  We can give a boolean expression to the slicing operator of a dataframe (that's `X[<...>]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[X.isnull().any(axis=1)] # Which rows have NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.columns[pd.isnull(X).any()].tolist() # Which columns have NaN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The topic of what to do with missing data is a big one, and not something we can cover in a single cell.  The simplest solution is to [*impute*](https://en.wikipedia.org/wiki/Imputation_%28statistics%29) them to be some kind of average or otherwise representative value.  Here, I'll take the median of the column.  \n",
    "\n",
    "One important side note: when we use `df[<my_command>]` to slice up the dataset, we're getting back what's called a *view*; it's not the original data.  That is, `X` strictly speaking is **not** precisely some of the columns of `df`.  Therefore,  setting individual values in a dataframe can sometimes be strange if we've done any of those slices (and we have!).  Pandas will yell at you with a warning about this, though for something this simple it will probably work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This one will (probably!) work but will (probably!) raise a warning:\n",
    "X.loc[18,\"WaterSamples\"] = X[\"WaterSamples\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is happening because Pandas can't guarantee that it's done what you've asked it. In order to be sure that you've got a fully functioning `DataFrame` object, first make it so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Look, no warnings!\n",
    "X.loc[18,\"SocialNetwork\"] = X[\"SocialNetwork\"].median()\n",
    "X.loc[18,\"MartianLandscape\"] = X[\"MartianLandscape\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's take a look to see that it did indeed all work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X[X.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  Now, of all those different datasets, which ones scored the highest (as in, which ones did students want to work with the most)?  We need to find a way to select those rows that correspond to jobs.  It would be a lot of work to type them all out!  Is there anything else we can use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_datasets = list(club_groups.keys()) + list(job_groups.keys()) + [\"Period\"]\n",
    "datasets = [col for col in X.columns if col not in not_datasets]\n",
    "print(*datasets, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked!  Now let's see them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totals = X[datasets].sum(axis=0).sort_values()\n",
    "plt.figure(figsize=(10, 8))\n",
    "totals.plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting!  I wasn't expecting emails to win.  Or really any of those top 5 or so.  The last thing we'll do with this elementary analysis is to look at basic statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There's so, so, so much more we can do with this dataset, now that we have it in a form we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### footer_latex\n",
    "$\\LaTeX$ is the *de facto* standard tool for typesetting mathematics and all things based upon in (*e.g.* science).  You separate your mathematical code using dollar signs, as you saw me do up there.  By the way, the math in that example above was both correct and adorable (Can you tell me why?).\n",
    "\n",
    "### footer_list_comprehensions\n",
    "What does that `print` statement above mean? Well, let's start with something easier:\n",
    "```\n",
    ">>> [x for x in range(2, 401)]\n",
    "```\n",
    "This is equal to [2, 3, 4, ..., 400] (a list!).  This is called a [list comprehension](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions).\n",
    "```\n",
    ">>> [x for x in range(2, 401) if is_prime(x)]\n",
    "```\n",
    "This is all the primes between 2 and 400\n",
    "```\n",
    ">>> print(*[<that_command_above>])\n",
    "```\n",
    "The asterisk is sometimes called the *splat operator*, it \"unpacks\" the list.  Thus, `my_func(*[2,3,4])` is equivalent to `my_func(2,3,4)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
