{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Basics of Natural Language Processing\n",
    "\n",
    "### Nicholas Zufelt, CSC630 Data Visualization\n",
    "\n",
    "In this notebook, we use the `nltk` library to vectorize the text \"Siddhartha\" by Herman Hesse.\n",
    "\n",
    "The main goal of vectorization is to take [natural language](https://en.wikipedia.org/wiki/Natural_language) and \"turn it a data set\".  What that means depends on how you plan to use it, but for us, we'll take a common, basic approach:\n",
    "\n",
    "**Given a novel, break it into parts (sections, chapters, etc.) and then into sentences.  Turn each sentence into a vector of length $N$ consisting of 1's and 0's, given by whether the top $N$ non-stop words are contained in each sentence.**\n",
    "\n",
    "There's a lot in there:\n",
    "1. Break the text into pieces appropriately,\n",
    "2. Break each part into sentences,\n",
    "3. Remove _stop words_, _i.e._ words that are commonly present in most natural language and which don't add much meaning to the text (e.g. `the`, `is`, _etc._).  What is known as a stop words is often based upon the text itself, and requires some fine-tuning.\n",
    "4. Determine the top $N$ non-stop words, deciding upon $N$ by some kind of process.  This typically involves _stemming_ the words, _i.e._ reducing words to their stem: `charge`, `charging`, `charged` all become `charg`.\n",
    "5. Replace each sentence with the appropriate vector.  This is often called the _bag-of-words_ approach, because it doesn't take into account the interaction between words: having the word \"peanut\" in a sentence increases the chancese of having the word \"butter\" in the sentence, for example, and we'll disregard such concerns here.\n",
    "\n",
    "Now, in our line of work, doing all that by hand would take a lot of lines of Python. Fortunately, the Natural Language Toolkit (`nltk`) is a Python library that has many built-in, helpful tools for these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\cdeme\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\cdeme\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\cdeme\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll likely need to install nltk the first time you use it.  Recall that you can do that inside of this notebook!  You use \n",
    "\n",
    "```\n",
    "!pip install nltk\n",
    "```\n",
    "\n",
    "to have [pip](https://en.wikipedia.org/wiki/Pip_%28package_manager%29) install it for you.\n",
    "\n",
    "## Removing non-content\n",
    "\n",
    "Okay, let's get a book from [Project Gutenberg](http://www.gutenberg.org/), as an example to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/cache/epub/2500/pg2500.txt\"\n",
    "\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we're working with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Siddhartha, by Herman Hesse\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: Siddhartha\r\n",
      "\r\n",
      "Author: Herman Hesse\r\n",
      "\r\n",
      "Translator: Gunther Olesch, Anke Dreher, Amy Coulter, Stefan Langer and Semyon Chaichenets\r\n",
      "\r\n",
      "Release Date: April 6, 2008 [EBook #2500]\r\n",
      "Last updated: July 2, 2011\r\n",
      "Last updated: January 23, 2013\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "\r\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK SIDDHARTHA ***\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Produced by Michael Pullen,  Chandra Yenco, Isaac Jones\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "SIDDHARTHA\r\n",
      "\r\n",
      "An Indian Tale\r\n",
      "\r\n",
      "by Hermann Hesse\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "FIRST PART\r\n",
      "\r\n",
      "To Romain Rolland, my dear friend\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "THE SON OF THE BRAHMAN\r\n",
      "\r\n",
      "In the shade of the house, in the sunshine of the riverbank near the\r\n",
      "boats, in the shade of the Sal-wood forest, in the shade of the fig\n"
     ]
    }
   ],
   "source": [
    "print(res.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there's some text in the beginning that we don't want to include in our analysis of the text, ending with \n",
    "\n",
    "```\n",
    "By Hermann Hesse\n",
    "```\n",
    "\n",
    "Let's remove up to that, and then see what's at the end, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST PART\r\n",
      "\r\n",
      "To Romain Rolland, my dear friend\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "THE SON OF THE BRAHMAN\r\n",
      "\r\n",
      "In the shade of t\n"
     ]
    }
   ],
   "source": [
    "i = res.text.index(\"FIRST PART\")\n",
    "\n",
    "text = res.text[i:]\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s innermost self, Govinda\r\n",
      "still stood for a little while bent over Siddhartha's quiet face, which\r\n",
      "he had just kissed, which had just been the scene of all manifestations,\r\n",
      "all transformations, all existence.  The face was unchanged, after under\r\n",
      "its surface the depth of the thousandfoldness had closed up again, he\r\n",
      "smiled silently, smiled quietly and softly, perhaps very benevolently,\r\n",
      "perhaps very mockingly, precisely as he used to smile, the exalted one.\r\n",
      "\r\n",
      "Deeply, Govinda bowed; tears he knew nothing of, ran down his old face;\r\n",
      "like a fire burnt the feeling of the most intimate love, the humblest\r\n",
      "veneration in his heart.  Deeply, he bowed, touching the ground, before\r\n",
      "him who was sitting motionlessly, whose smile reminded him of everything\r\n",
      "he had ever loved in his life, what had ever been valuable and holy to\r\n",
      "him in his life.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "End of the Project Gutenberg EBook of Siddhartha, by Herman Hesse\r\n",
      "\r\n",
      "*** END OF THIS PROJECT GUTENBERG EBOOK SIDDHARTHA ***\r\n",
      "\r\n",
      "***** This file \n"
     ]
    }
   ],
   "source": [
    "# Tinkering with the numbers below brought me to:\n",
    "print(text[-20000:-19000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the end of the book is signaled by \n",
    "\n",
    "```\n",
    "End of the Project Gutenberg EBook of Siddhartha, by Herman Hesse\n",
    "```\n",
    "So we'll strip that off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he had ever loved in his life, what had ever been valuable and holy to\r\n",
      "him in his life.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = text.index(\"End of the Project Gutenberg EBook\")\n",
    "\n",
    "text = text[:i]\n",
    "print(text[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now our text is ready.  \n",
    "\n",
    "## Breaking into parts/chapters\n",
    "\n",
    "I also saw that the big sections/chapters, etc are written in all caps.  So to split it up by chapters, I'm going to use our first part of `nltk`, a _tokenizer_.  A tokenizer is an object which can break up natural language in a way you specify.  For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'cat', 'in', 'the', 'hat', 'sat', 'on', 'the', 'bat,', 'then', 'spat.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(\"The cat in the hat sat on the bat, then spat.\")\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer we'll use first is `RegexpTokenizer`, which takes in its constructor a regular expression pattern to match.  Here, we'll use the pattern \n",
    "```\n",
    "^(?:[A-Z]+ ?)+\n",
    "```\n",
    "I tweaked this until I got what I wanted.  Use [this website](https://regex101.com/) to help fine-tune your regular expression to be what you want.  Since regular expressions are an [entire problem of themselves](http://regex.info/blog/2006-09-15/247), I won't spend time explaining them.  Just ask for help if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "# grab all lines that are all caps\n",
    "re_tokenizer = RegexpTokenizer(\"^(?:[A-Z][A-Z]+ ?)+\")\n",
    "sections = re_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FIRST PART',\n",
       " 'THE SON OF THE BRAHMAN',\n",
       " 'WITH THE SAMANAS',\n",
       " 'GOTAMA',\n",
       " 'AWAKENING',\n",
       " 'SECOND PART',\n",
       " 'KAMALA',\n",
       " 'WITH THE CHILDLIKE PEOPLE',\n",
       " 'SANSARA',\n",
       " 'BY THE RIVER',\n",
       " 'THE FERRYMAN',\n",
       " 'THE SON',\n",
       " 'OM',\n",
       " 'GOVINDA']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That `\"OM\"` looks suspect to me, so I want to check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy found him asleep.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "OM\r\n",
      "\r\n",
      "For a long time, the wou\n"
     ]
    }
   ],
   "source": [
    "i = text.index(\"OM\")\n",
    "\n",
    "print(text[i-30:i+30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it actually seems fine.  What a funny name for a chapter header!  At any rate, we can break the text into chapters using this tokenizer.  We can use `span_tokenize` to get the starting and ending indices of the titles, and then `zip` to \"zip together\" the two different pieces of information.  Finally, `zip` returns a generator, so we can \"exhaust\" the  generator by making it become a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FIRST PART', (0, 10))\n",
      "('THE SON OF THE BRAHMAN', (57, 79))\n",
      "('WITH THE SAMANAS', (15469, 15485))\n",
      "('GOTAMA', (34498, 34504))\n",
      "('AWAKENING', (52193, 52202))\n",
      "('SECOND PART', (60455, 60466))\n",
      "('KAMALA', (60528, 60534))\n",
      "('WITH THE CHILDLIKE PEOPLE', (86857, 86882))\n",
      "('SANSARA', (103517, 103524))\n",
      "('BY THE RIVER', (121626, 121638))\n",
      "('THE FERRYMAN', (144093, 144105))\n",
      "('THE SON', (168034, 168041))\n",
      "('OM', (186037, 186039))\n",
      "('GOVINDA', (199600, 199607))\n"
     ]
    }
   ],
   "source": [
    "print(*zip(re_tokenizer.tokenize(text), re_tokenizer.span_tokenize(text)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's _almost_ what I want.  What I have here is the beginning and ending indices of the _chapter titles_, and I want the beginning and ending indices of the _chapters_.  That's an easy fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FIRST PART', [10, 57]]\n",
      "['THE SON OF THE BRAHMAN', [79, 15469]]\n",
      "['WITH THE SAMANAS', [15485, 34498]]\n",
      "['GOTAMA', [34504, 52193]]\n",
      "['AWAKENING', [52202, 60455]]\n",
      "['SECOND PART', [60466, 60528]]\n",
      "['KAMALA', [60534, 86857]]\n",
      "['WITH THE CHILDLIKE PEOPLE', [86882, 103517]]\n",
      "['SANSARA', [103524, 121626]]\n",
      "['BY THE RIVER', [121638, 144093]]\n",
      "['THE FERRYMAN', [144105, 168034]]\n",
      "['THE SON', [168041, 186037]]\n",
      "['OM', [186039, 199600]]\n",
      "['GOVINDA', [199607, 0]]\n"
     ]
    }
   ],
   "source": [
    "chapters_by_title_indices = list(zip(re_tokenizer.tokenize(text), re_tokenizer.span_tokenize(text)))\n",
    "\n",
    "chapters = []\n",
    "\n",
    "for i, chapter in enumerate(chapters_by_title_indices):\n",
    "    # Carry over the chapter title\n",
    "    c = [chapter[0], [chapter[1][1], 0]]\n",
    "    try: \n",
    "        # grab the starting index of the next chapter, if it exists\n",
    "        c[1][1] = chapters_by_title_indices[i+1][1][0]\n",
    "    except:\n",
    "        # it didn't work, meaning we're at the end.\n",
    "        pass\n",
    "    chapters.append(c)\n",
    "    \n",
    "print(*chapters, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're ready to roll!  After we do this, all the new lines are not useful, so we'll get rid of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('THE SON OF THE BRAHMAN',\n",
       " '  In the shade of the house, in the sunshine of the riverbank near the boats, in the shade of the Sal-wood forest, in the shade of the fig tree is where Siddhartha grew up, the handsome son of the Brahman, the young falcon, together with his friend Govinda, son of a Brahman.  The sun tanned his light shoulders by the banks of the river when bathing, performing the sacred ablutions, the sacred offerings.  In the mango grove, shade poured into his black eyes, when playing as a boy, when his mother sang, when the sacred offerings were made, when his father, the scholar, taught him, when the wise men talked.  For a long time, Siddhartha had been partaking in the discussions of the wise men, practising debate with Govinda, practising with Govinda the art of reflection, the service of meditation.  He already knew how to speak the Om silently, the word of words, to speak it silently into himself while inhaling, to speak it silently out of himself while exhaling, with all the concentration of his soul, the forehead surrounded by the glow of the clear-thinking spirit.  He already knew to feel Atman in the depths of his being, indestructible, one with the universe.  Joy leapt in his father\\'s heart for his son who was quick to learn, thirsty for knowledge; he saw him growing up to become great wise man and priest, a prince among the Brahmans.  Bliss leapt in his mother\\'s breast when she saw him, when she saw him walking, when she saw him sit down and get up, Siddhartha, strong, handsome, he who was walking on slender legs, greeting her with perfect respect.  Love touched the hearts of the Brahmans\\' young daughters when Siddhartha walked through the lanes of the town with the luminous forehead, with the eye of a king, with his slim hips.  But more than all the others he was loved by Govinda, his friend, the son of a Brahman.  He loved Siddhartha\\'s eye and sweet voice, he loved his walk and the perfect decency of his movements, he loved everything Siddhartha did and said and what he loved most was his spirit, his transcendent, fiery thoughts, his ardent will, his high calling. Govinda knew: he would not become a common Brahman, not a lazy official in charge of offerings; not a greedy merchant with magic spells; not a vain, vacuous speaker; not a mean, deceitful priest; and also not a decent, stupid sheep in the herd of the many.  No, and he, Govinda, as well did not want to become one of those, not one of those tens of thousands of Brahmans.  He wanted to follow Siddhartha, the beloved, the splendid.  And in days to come, when Siddhartha would become a god, when he would join the glorious, then Govinda wanted to follow him as his friend, his companion, his servant, his spear-carrier, his shadow.  Siddhartha was thus loved by everyone.  He was a source of joy for everybody, he was a delight for them all.  But he, Siddhartha, was not a source of joy for himself, he found no delight in himself.  Walking the rosy paths of the fig tree garden, sitting in the bluish shade of the grove of contemplation, washing his limbs daily in the bath of repentance, sacrificing in the dim shade of the mango forest, his gestures of perfect decency, everyone\\'s love and joy, he still lacked all joy in his heart.  Dreams and restless thoughts came into his mind, flowing from the water of the river, sparkling from the stars of the night, melting from the beams of the sun, dreams came to him and a restlessness of the soul, fuming from the sacrifices, breathing forth from the verses of the Rig-Veda, being infused into him, drop by drop, from the teachings of the old Brahmans.  Siddhartha had started to nurse discontent in himself, he had started to feel that the love of his father and the love of his mother, and also the love of his friend, Govinda, would not bring him joy for ever and ever, would not nurse him, feed him, satisfy him.  He had started to suspect that his venerable father and his other teachers, that the wise Brahmans had already revealed to him the most and best of their wisdom, that they had already filled his expecting vessel with their richness, and the vessel was not full, the spirit was not content, the soul was not calm, the heart was not satisfied.  The ablutions were good, but they were water, they did not wash off the sin, they did not heal the spirit\\'s thirst, they did not relieve the fear in his heart.  The sacrifices and the invocation of the gods were excellent--but was that all?  Did the sacrifices give a happy fortune?  And what about the gods? Was it really Prajapati who had created the world?  Was it not the Atman, He, the only one, the singular one?  Were the gods not creations, created like me and you, subject to time, mortal?  Was it therefore good, was it right, was it meaningful and the highest occupation to make offerings to the gods?  For whom else were offerings to be made, who else was to be worshipped but Him, the only one, the Atman?  And where was Atman to be found, where did He reside, where did his eternal heart beat, where else but in one\\'s own self, in its innermost part, in its indestructible part, which everyone had in himself?  But where, where was this self, this innermost part, this ultimate part?  It was not flesh and bone, it was neither thought nor consciousness, thus the wisest ones taught.  So, where, where was it?  To reach this place, the self, myself, the Atman, there was another way, which was worthwhile looking for?  Alas, and nobody showed this way, nobody knew it, not the father, and not the teachers and wise men, not the holy sacrificial songs!  They knew everything, the Brahmans and their holy books, they knew everything, they had taken care of everything and of more than everything, the creation of the world, the origin of speech, of food, of inhaling, of exhaling, the arrangement of the senses, the acts of the gods, they knew infinitely much--but was it valuable to know all of this, not knowing that one and only thing, the most important thing, the solely important thing?  Surely, many verses of the holy books, particularly in the Upanishades of Samaveda, spoke of this innermost and ultimate thing, wonderful verses.  \"Your soul is the whole world\", was written there, and it was written that man in his sleep, in his deep sleep, would meet with his innermost part and would reside in the Atman.  Marvellous wisdom was in these verses, all knowledge of the wisest ones had been collected here in magic words, pure as honey collected by bees.  No, not to be looked down upon was the tremendous amount of enlightenment which lay here collected and preserved by innumerable generations of wise Brahmans.-- But where were the Brahmans, where the priests, where the wise men or penitents, who had succeeded in not just knowing this deepest of all knowledge but also to live it?  Where was the knowledgeable one who wove his spell to bring his familiarity with the Atman out of the sleep into the state of being awake, into the life, into every step of the way, into word and deed?  Siddhartha knew many venerable Brahmans, chiefly his father, the pure one, the scholar, the most venerable one.  His father was to be admired, quiet and noble were his manners, pure his life, wise his words, delicate and noble thoughts lived behind its brow --but even he, who knew so much, did he live in blissfulness, did he have peace, was he not also just a searching man, a thirsty man?  Did he not, again and again, have to drink from holy sources, as a thirsty man, from the offerings, from the books, from the disputes of the Brahmans? Why did he, the irreproachable one, have to wash off sins every day, strive for a cleansing every day, over and over every day?  Was not Atman in him, did not the pristine source spring from his heart?  It had to be found, the pristine source in one\\'s own self, it had to be possessed!  Everything else was searching, was a detour, was getting lost.  Thus were Siddhartha\\'s thoughts, this was his thirst, this was his suffering.  Often he spoke to himself from a Chandogya-Upanishad the words: \"Truly, the name of the Brahman is satyam--verily, he who knows such a thing, will enter the heavenly world every day.\"  Often, it seemed near, the heavenly world, but never he had reached it completely, never he had quenched the ultimate thirst.  And among all the wise and wisest men, he knew and whose instructions he had received, among all of them there was no one, who had reached it completely, the heavenly world, who had quenched it completely, the eternal thirst.  \"Govinda,\" Siddhartha spoke to his friend, \"Govinda, my dear, come with me under the Banyan tree, let\\'s practise meditation.\"  They went to the Banyan tree, they sat down, Siddhartha right here, Govinda twenty paces away.  While putting himself down, ready to speak the Om, Siddhartha repeated murmuring the verse:  Om is the bow, the arrow is soul, The Brahman is the arrow\\'s target, That one should incessantly hit.  After the usual time of the exercise in meditation had passed, Govinda rose.  The evening had come, it was time to perform the evening\\'s ablution. He called Siddhartha\\'s name.  Siddhartha did not answer.  Siddhartha sat there lost in thought, his eyes were rigidly focused towards a very distant target, the tip of his tongue was protruding a little between the teeth, he seemed not to breathe.  Thus sat he, wrapped up in contemplation, thinking Om, his soul sent after the Brahman as an arrow.  Once, Samanas had travelled through Siddhartha\\'s town, ascetics on a pilgrimage, three skinny, withered men, neither old nor young, with dusty and bloody shoulders, almost naked, scorched by the sun, surrounded by loneliness, strangers and enemies to the world, strangers and lank jackals in the realm of humans.  Behind them blew a hot scent of quiet passion, of destructive service, of merciless self-denial.  In the evening, after the hour of contemplation, Siddhartha spoke to Govinda:  \"Early tomorrow morning, my friend, Siddhartha will go to the Samanas.  He will become a Samana.\"  Govinda turned pale, when he heard these words and read the decision in the motionless face of his friend, unstoppable like the arrow shot from the bow.  Soon and with the first glance, Govinda realized:  Now it is beginning, now Siddhartha is taking his own way, now his fate is beginning to sprout, and with his, my own.  And he turned pale like a dry banana-skin.  \"O Siddhartha,\" he exclaimed, \"will your father permit you to do that?\"  Siddhartha looked over as if he was just waking up.  Arrow-fast he read in Govinda\\'s soul, read the fear, read the submission.  \"O Govinda,\" he spoke quietly, \"let\\'s not waste words.  Tomorrow, at daybreak I will begin the life of the Samanas.  Speak no more of it.\"  Siddhartha entered the chamber, where his father was sitting on a mat of bast, and stepped behind his father and remained standing there, until his father felt that someone was standing behind him.  Quoth the Brahman:  \"Is that you, Siddhartha?  Then say what you came to say.\"  Quoth Siddhartha: \"With your permission, my father.  I came to tell you that it is my longing to leave your house tomorrow and go to the ascetics.  My desire is to become a Samana.  May my father not oppose this.\"  The Brahman fell silent, and remained silent for so long that the stars in the small window wandered and changed their relative positions, \\'ere the silence was broken.  Silent and motionless stood the son with his arms folded, silent and motionless sat the father on the mat, and the stars traced their paths in the sky.  Then spoke the father:  \"Not proper it is for a Brahman to speak harsh and angry words.  But indignation is in my heart.  I wish not to hear this request for a second time from your mouth.\"  Slowly, the Brahman rose; Siddhartha stood silently, his arms folded.  \"What are you waiting for?\" asked the father.  Quoth Siddhartha:  \"You know what.\"  Indignant, the father left the chamber; indignant, he went to his bed and lay down.  After an hour, since no sleep had come over his eyes, the Brahman stood up, paced to and fro, and left the house.  Through the small window of the chamber he looked back inside, and there he saw Siddhartha standing, his arms folded, not moving from his spot.  Pale shimmered his bright robe.  With anxiety in his heart, the father returned to his bed.  After another hour, since no sleep had come over his eyes, the Brahman stood up again, paced to and fro, walked out of the house and saw that the moon had risen.  Through the window of the chamber he looked back inside; there stood Siddhartha, not moving from his spot, his arms folded, moonlight reflecting from his bare shins.  With worry in his heart, the father went back to bed.  And he came back after an hour, he came back after two hours, looked through the small window, saw Siddhartha standing, in the moon light, by the light of the stars, in the darkness.  And he came back hour after hour, silently, he looked into the chamber, saw him standing in the same place, filled his heart with anger, filled his heart with unrest, filled his heart with anguish, filled it with sadness.  And in the night\\'s last hour, before the day began, he returned, stepped into the room, saw the young man standing there, who seemed tall and like a stranger to him.  \"Siddhartha,\" he spoke, \"what are you waiting for?\"  \"You know what.\"  \"Will you always stand that way and wait, until it\\'ll becomes morning, noon, and evening?\"  \"I will stand and wait.  \"You will become tired, Siddhartha.\"  \"I will become tired.\"  \"You will fall asleep, Siddhartha.\"  \"I will not fall asleep.\"  \"You will die, Siddhartha.\"  \"I will die.\"  \"And would you rather die, than obey your father?\"  \"Siddhartha has always obeyed his father.\"  \"So will you abandon your plan?\"  \"Siddhartha will do what his father will tell him to do.\"  The first light of day shone into the room.  The Brahman saw that Siddhartha was trembling softly in his knees.  In Siddhartha\\'s face he saw no trembling, his eyes were fixed on a distant spot.  Then his father realized that even now Siddhartha no longer dwelt with him in his home, that he had already left him.  The Father touched Siddhartha\\'s shoulder.  \"You will,\" he spoke, \"go into the forest and be a Samana.  When you\\'ll have found blissfulness in the forest, then come back and teach me to be blissful.  If you\\'ll find disappointment, then return and let us once again make offerings to the gods together.  Go now and kiss your mother, tell her where you are going to.  But for me it is time to go to the river and to perform the first ablution.\"  He took his hand from the shoulder of his son and went outside. Siddhartha wavered to the side, as he tried to walk.  He put his limbs back under control, bowed to his father, and went to his mother to do as his father had said.  As he slowly left on stiff legs in the first light of day the still quiet town, a shadow rose near the last hut, who had crouched there, and joined the pilgrim--Govinda.  \"You have come,\" said Siddhartha and smiled.  \"I have come,\" said Govinda.    ')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into the two parts of the text\n",
    "sectionized_text = [[], []]\n",
    "\n",
    "part = -1\n",
    "for chapter in chapters:\n",
    "    if \"PART\" in chapter[0]:\n",
    "        # this is not a chapter\n",
    "        part += 1\n",
    "        continue\n",
    "    else:\n",
    "        chapter_text = text[chapter[1][0]: chapter[1][1]-1]\n",
    "        sectionized_text[part].append(\n",
    "            (chapter[0],\n",
    "             chapter_text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "            ))\n",
    "        \n",
    "sectionized_text[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our text broken into chapters, and now let's break each chapter into sentences.\n",
    "\n",
    "## Sentences as lists of words\n",
    "\n",
    "In order to have the following sections work, you may need to install three things from nltk: \n",
    "1. the [`punkt`](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.5017&rep=rep1&type=pdf) sentence-finder, \n",
    "2. the nltk stopwords corpus, and \n",
    "3. the `wordnet` \n",
    "\n",
    "Why do I need a sentence-finder tool (the other parts will be clear later)?  Well, to quote [Wikipedia](https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation):\n",
    "\n",
    "> Sentence boundary disambiguation (SBD), also known as sentence breaking or sentence boundary detection, is the problem in natural language processing of deciding where sentences begin and end. Often, natural language processing tools require their input to be divided into sentences for a number of reasons; however, sentence boundary identification is challenging because punctuation marks are often ambiguous. For example, a period may denote an abbreviation, decimal point, an ellipsis, or an email address – not the end of a sentence. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. As well, question marks and exclamation marks may appear in embedded quotations, emoticons, computer code, and slang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After running this cell, a dialog window should open:\n",
    "# Under \"Models\", download \"punkt\",\n",
    "# Under \"Corpora\", download \"stopwords\" and \"wordnet\"\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cdeme/nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9ed848bf2e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# just to make sure it's working:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msectionized_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cdeme/nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# just to make sure it's working:\n",
    "nltk.sent_tokenize(sectionized_text[0][0][1])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, since our text has been split into sentences, we now want to split into words.  You might want to say \"okay, just do `my_sentence.split(' ')`\", but then we'll get strange punctuation in some of our words.  Okay, then how about removing all the punctuation?  Well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation      # this seems useful!  Let's use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Immediately he couldnt believe his eyes what a feast'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Immediately, he couldn't believe his eyes; what a feast!\"\n",
    "for punc in string.punctuation:\n",
    "    sentence = sentence.replace(punc, \"\")\n",
    "    \n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That's not bad, but `could` and `couldn't` are very different words, and I fear that we need to separate those if possible.  Fortunately, `nltk` has a tool for us, again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cdeme/nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0e25e9eae2b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Immediately, he couldn't believe his eyes; what a feast!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     return [\n\u001b[0;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\cdeme/nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\cdeme\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Immediately, he couldn't believe his eyes; what a feast!\"\n",
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I can remove those \"words\" that are just punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(sentence)\n",
    "for i, word in enumerate(word_tokens):\n",
    "    if len(word) == 1 and word[0] in string.punctuation:\n",
    "        word_tokens.pop(i)\n",
    "        \n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words and vectorizing\n",
    "\n",
    "The last step before creating a vector for each sentence is to remove all the useless words.  These words include things like `\"the\"` and `\"a\"`, but there are a lot more than that.  They're called stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:15], len(stopwords.words('english')[:15]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems perfect, but this is often a stage where you need to look through and see if you want to keep some of them, or add some of your own.  It might not be what you want, but it's a useful thing to start from!  To remove them, you do the most obvious thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_word_tokens = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "useful_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now to create a vector.  The idea here is to create a _dictionary_ of the most common useful (non-stop) words, and then for each sentence, record whether that word appears in that sentence.  The useful tool here is `collections.Counter`, which is a (Python) dictionary which records each time you add an item to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "dictionary = Counter()\n",
    "dictionary.update(useful_word_tokens + ['eyes', 'eyes', 'feast'])\n",
    "    \n",
    "print(dictionary)\n",
    "print(dictionary.most_common(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems perfect.  Let's make a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fe4a9cefd6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0mThis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mabove\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtokenizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentence\u001b[0m \u001b[0minto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = Counter()\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    This is just the code from above that tokenizes the sentence into \n",
    "    words, then deletes things like `.` and removes stop words, wrapped \n",
    "    in a function.\n",
    "    \"\"\"\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    for i, word in enumerate(word_tokens):\n",
    "        if len(word) == 1 and word[0] in string.punctuation:\n",
    "            word_tokens.pop(i)\n",
    "\n",
    "    return [word.lower() for word in word_tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "\n",
    "for part in sectionized_text:\n",
    "    for chapter in part:\n",
    "        # recall that `chapter` is a tuple, `(chapter_name, chapter_text)`\n",
    "        sentences = nltk.sent_tokenize(chapter[1])\n",
    "        sentences_in_word_tokens = [tokenize_sentence(sentence) for sentence in sentences]\n",
    "        for tokenized_sentence in sentences_in_word_tokens:\n",
    "            dictionary.update(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9786b06cd93c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you might notice that it isn't perfect, but there's some good stuff there!  \n",
    "\n",
    "Some things to add to the picture, that I'll leave as an: \n",
    "##### Exercise\n",
    "1. Make a small \"lookup table\" to turn things like `\"n't\"` into \"not\",\n",
    "2. Remove random things like `\"''\"`,\n",
    "3. Tweak your list of stopwords from the default to include or don't include some words.\n",
    "4. [Stem your words](#Stemming) \n",
    "\n",
    "For now, I'll just use the top 200 words as my dictionary (with the above errors not fixed, as this is already taking a while), and vectorize the sentence like that.  To \"vectorize\", just replace the sentence with a length-200 array of `1`'s and `0`'s corresponding to whether the top `i`th word appears in the sentence.\n",
    "\n",
    "For example, if my dictionary were only 6 words (instead of 200): \n",
    "```\n",
    "[\"food\", \"glorious\", \"upon\", \"wonderment\", \"lark\", \"cunning\"]\n",
    "```\n",
    "and my sentence was `\"Hark, a lark dost descend upon me.\"`, then the sentences vector becomes `[0, 0, 1, 0, 1, 0]` (because `upon` and `lark` are in the dictionary and the sentence, but the other words of the dictionary are not in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c4e989b65285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary_200\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdictionary_200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary_200 = [word[0] for word in dictionary.most_common(200)]\n",
    "\n",
    "def vectorize(word_tokens):\n",
    "    vector = []\n",
    "    for word in dictionary_200:\n",
    "        if word in word_tokens:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    \n",
    "    return vector\n",
    "\n",
    "print(vectorize([\"i\", \"siddhartha\", \"am\", \"brave\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!  Let's finish off this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sectionized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_format = [[[], [], [], []],   # Part 1\n",
    "                     [[], [], [], [], [], [], [], []]      # Part 2\n",
    "                    ]\n",
    "\n",
    "for part_ind, part in enumerate(sectionized_text):\n",
    "    for chap_ind, chapter in enumerate(part):\n",
    "        # recall that `chapter` is a tuple, `(chapter_name, chapter_text)`\n",
    "        sentences = nltk.sent_tokenize(chapter[1])\n",
    "        sentences_in_word_tokens = [tokenize_sentence(sentence) for sentence in sentences]\n",
    "        for tokenized_sentence in sentences_in_word_tokens:\n",
    "            final_text_format[part_ind][chap_ind].append(vectorize(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first three sentences, vectorized\n",
    "print(*final_text_format[0][0][:3], sep=\"\\n\\n\")\n",
    "\n",
    "with open('siddhartha.json', 'w') as f:\n",
    "    json.dump(final_text_format, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now ready for whatever analysis you want to perform ... with the [above caveats](#Exercise), along with choosing an appropriate length for your dictionary, taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Options\n",
    "### Stemming\n",
    "You'll probably want to **stem** the words: remove the differences that are from the same part of speech so that things like \"running\", \"run\", \"ran\", etc. all (hopefully!) become the same \"word stem\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-50c5fa98ce12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Quickly running cats start jumping on birds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for word in tokenize_sentence(\"Quickly running cats start jumping on birds\"):\n",
    "    print(word, \":\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked really well!  It does indeed have issues, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"flying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part of speech tagging\n",
    "\n",
    "Check out [this link](http://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger) for an explanation of how to do part of speech tagging, if that's something you'd like to try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "\n",
    "Instead of stemming, you might try your hand at **lemmatizing**: turning all the words of a certain stem into a single, usable, form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"fairly\", \"running\", \"quicker\", \"dogs\", \"fishes\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \":\", lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not always successful.  It sometimes helps if you add the part of speech tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos options are 'n' for noun, 'v' for verb, 'r' for adverb, 'a' for adjective\n",
    "print(\"running\", \":\", lemmatizer.lemmatize(\"running\", pos='v'))\n",
    "\n",
    "# but it doesn't always work:\n",
    "for part in ['n', 'v', 'r', 'a', 's']:\n",
    "    print(\"fairly\", \":\", lemmatizer.lemmatize(\"fairly\", pos=part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
